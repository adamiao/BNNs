# BNNs

This project is an exploration of how to implemente a gradient boosted algorithm where the weak learners being used are neural networks rather than decision trees.
The main purpose is to get a better understanding of neural networks and gradient boosting.

Disclaimer: not all of the written code is originally mine. The portion of code which contains the neural network framework was originally devised by 
Michael Nielsen. A great tutorial made by him (with python code!) can be found at: http://neuralnetworksanddeeplearning.com/chap1.html. However, since I started working 
on this project, changes have been made to this portion of the code. These changes were mainly motivated by me trying to understand the underlying theory of neural networks.
For example, I wanted to see how one would go about adding regularization terms and how that would affect overall performance of the method.

The gradient boosting portion of the code was completely coded by me based on what I understood from the theory.